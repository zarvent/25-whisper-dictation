üó£Ô∏è **herramienta de dictado por voz (refactorizada)**

---

üéØ **prop√≥sito**

el prop√≥sito es simple: poder dictar texto en cualquier lugar del sistema operativo.

la idea es que puedas transcribir audio por GPU (para m√°xima velocidad) en cualquier campo de texto, sin importar la aplicaci√≥n.

este proyecto es una refactorizaci√≥n de un script simple que ten√≠a. lo mov√≠ a una aplicaci√≥n modular de Python para separar las responsabilidades (transcripci√≥n vs. l√≥gica de LLM) y hacerlo mucho m√°s f√°cil de mantener y configurar a futuro.

---

üïπÔ∏è **interacci√≥n y flujo de trabajo**

la interacci√≥n se divide en dos funciones principales. ambas est√°n pensadas para ser activadas con atajos de teclado globales, para que no interrumpan tu flujo de trabajo.

**1. flujo de dictado (voz ‚Üí texto)**

este es el flujo principal: capturar tu voz y convertirla en texto. est√° activado por `scripts/whisper-toggle.sh`.

```mermaid
flowchart TD
    A[‚è∫Ô∏è atajo (ej. ctrl+may√∫s+espacio)] --> B(inicia grabaci√≥n üé§);
    B --> C[‚èπÔ∏è atajo (mismo)];
    C --> D{transcribe con Whisper};
    D --> E[üìã copiado al portapapeles];
```

**2. flujo de refinado (texto ‚Üí texto mejorado)**

a veces la transcripci√≥n no es perfecta. este flujo toma el texto de tu portapapeles y usa un LLM para limpiarlo, corregirlo o formatearlo. est√° activado por `scripts/process-clipboard.sh`.

```mermaid
flowchart TD
    A[üìã copias texto] --> B(üß† atajo secundario);
    B --> C{procesa con LLM (Gemini)};
    C --> D[üìã reemplaza portapapeles];
```

---

üß© **el n√∫cleo: de scripts a aplicaci√≥n**

aqu√≠ est√° el cambio principal de la refactorizaci√≥n. esto ya no es un simple script; es una aplicaci√≥n de Python que sigue principios de dise√±o m√°s robustos. decid√≠ usar inyecci√≥n de dependencias (DI) y un bus de comandos (un CQRS liviano) para orquestar los diferentes servicios. esto hace que el sistema sea mucho m√°s desacoplado y f√°cil de probar o extender.

**archivos y directorios clave:**

*   `src/whisper_dictation/main.py`: es el 'controlador' principal. escucha los comandos (`start`, `stop`, `process`) desde los scripts de shell.
*   `src/whisper_dictation/core/di/container.py`: aqu√≠ es donde se 'cablea' todo. define qu√© implementaci√≥n concreta se usa para cada interfaz (ej. 'usar Gemini para el servicio LLM').
*   `src/whisper_dictation/application/`: el 'cerebro' de la app. contiene la l√≥gica de negocio pura (los comandos y los handlers que saben qu√© hacer con ellos).
*   `src/whisper_dictation/infrastructure/`: las 'manos' de la app. aqu√≠ viven las implementaciones que hablan con el mundo real:
    *   `whisper_transcription_service.py`: se encarga de cargar `faster-whisper` y hacer la grabaci√≥n/transcripci√≥n.
    *   `gemini_llm_service.py`: maneja la conexi√≥n con la API de Google Gemini para el refinado.
*   `config.toml`: tu panel de control. aqu√≠ defines qu√© modelo de Whisper usar, si correr en `cuda` o `cpu`, y otros par√°metros.
*   `.env`: solo para tus secretos. aqu√≠ va tu `GEMINI_API_KEY`.

---

üõ†Ô∏è **instalaci√≥n y diagn√≥stico**

para poner esto en marcha, necesitas configurar tres capas: el sistema, Python y la IA.

**1. dependencias del sistema:**

primero, lo b√°sico del sistema. `ffmpeg` y `pactl` son para grabar, y `xclip` para copiar al portapapeles. si tienes una GPU NVIDIA, aseg√∫rate de que CUDA est√© listo.

*   aseg√∫rate de tener `ffmpeg`, `xclip` y `pactl`.
*   (opcional pero recomendado) drivers de NVIDIA y CUDA toolkit si usas `device: "cuda"`.

**2. entorno de Python:**

es una buena pr√°ctica usar un entorno virtual. esto mantiene las dependencias del proyecto aisladas.

```bash
# crear un entorno virtual
python3 -m venv venv

# activar el entorno
source venv/bin/activate

# instalar dependencias
pip install -r requirements.txt
```

**3. configuraci√≥n de IA (Gemini):**

para el refinado de texto, la app necesita tu clave de API de Gemini. la leemos desde un archivo `.env` para no 'quemarla' en el c√≥digo.

```bash
# crear el archivo .env si no existe
touch .env

# a√±adir tu api key de gemini al archivo .env
echo 'GEMINI_API_KEY="AIzaSy..."' > .env
```

**4. configuraci√≥n de la aplicaci√≥n:**

echa un vistazo a `config.toml`. aqu√≠ es donde puedes afinar el rendimiento, como elegir un modelo de Whisper m√°s peque√±o si `large-v2` es demasiado pesado.

*   revisa `config.toml`.
*   aseg√∫rate que `[whisper]` apunte al modelo y dispositivo correctos (ej. `model = "large-v2"`, `device = "cuda"`).

**5. verificaci√≥n:**

para asegurar que todo est√© conectado:

*   `scripts/verify-setup.sh` te da un chequeo r√°pido de las dependencias.
*   `python test_whisper_gpu.py` es √∫til para confirmar que `faster-whisper` est√° usando tu GPU (y no el CPU).

---

üìö **archivos hist√≥ricos**

si te interesa el 'c√≥mo' y 'por qu√©' de la evoluci√≥n de este proyecto, dej√© algunas notas en el archivo:

*   `archives/2025-11-16 MIGRATION.md` (detalles sobre la migraci√≥n de perplexity a Gemini).
*   `archives/2025-11-15 nueva feature.md` (justificaci√≥n de la feature de refinado).
*   `prompts/refine_system.txt` (el prompt exacto que usa Gemini).

---
*nota sobre la visualizaci√≥n en vs code: si los diagramas de flujo no se muestran, aseg√∫rate de tener instalada una extensi√≥n compatible con mermaid, como "markdown mermaid".*
